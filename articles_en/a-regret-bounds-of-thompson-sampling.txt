A regret bound of Thompson sampling
2022-07-01

@quote{Still in progress}

@p{In this post, I will explain the proof of the well-known regret bound of Thompson sampling described in this paper.}
@ul{A. Slivkins (2019), @a{Introduction to Multi-Armed Bandits}{https://arxiv.org/abs/1904.07272}, arXiv}
@p{The first half of this post will describe the definition of the bayesian bandit problem and their one of the solutions, Thompson sampling. The latter half will describe one of the key lemma, and the proof of the regret bound.}


@section{Description of the problem}

@p{The stochastic bandit problem is an representative example of explore-exploit tradeoff, in which a player selects one of @inmath{K} arms for @inmath{T} rounds. In each round @inmath{t}, the player receives a reward @inmath{r_t \in [0,1]} sampled from a fixed predetermined distribution @inmath{\mathcal{D}_{a_t}} depending on the arm he choose @inmath{a_t \in \mathcal{A}}. Here, the reward distribution @inmath{\mathcal{D}_a} of each arm @inmath{a} doesn't change throughout rounds, and doesn't correlate with the distribution of the other arms. In the following, I denote the mean reward of each arm as @inmath{\mu(a) = \mathbb{E}[\mathcal{D}_a]}, and the best expected reward as @inmath{\mu^* = \max_{a} \mu(a)}. The best arm @inmath{a = \argmax_{a} \mu(a)} is sometimes not well-defined because there may exist two or more arms with same expected reward. We define the best arm @inmath{a*} by choosing arbitary arm with the highest reward.}

@p{The bayesian bandit problem is an extension of the stochastic bandit problem, incorporating the player's prior knowledge about the arms and their rewards into the problem setup. The player is assumed to have some knowledge of the reward distribution before the first round, and the knowledge is expressed as a baysian prior @inmath{\mathbb{P}}, the probability distribution over the set of reward distributions @inmath{\{\mathcal{D}_a\}_{a \in \mathcal{A}}}.}

@p{The objective of the player in the bayesian bandit problem is to minimize the regret, defined as}
@blmath{ R(T) = \mu^* T - \sum_{t=1}^T \mu(a_t). }
@p{Because this depends on the instance of the problem the player interacts with, usual evaluation metric averages out the instance dependency by taking expectation over the bayesian prior @inmath{\mathbb{P}}}
@blmath{ BR(T) = \mathbb{E}[R(T)]. }


@section{Description of the algorithm}

@p{Before introducing the Thompson sampling algorithm, it is beneficial to define some notations to simplify the explanation. The t-history @inmath{H_t} is a trace of the player's interacttion with arms up to round @inmath{t}}
@blmath{H_t = \{(a_1, r_1), (a_2, r_2), \cdots, (a_t, r_t)\} \in (\mathcal{A} \times \mathbb{R})^t.}
@p{Depending on the algorithm and arms, some t-history will occur with a certain probability, while other may never occur because certain arm cannot give corresponding reward in the t-history. We call a sequence @inmath{H} of arms and rewards}
@blmath{H = \{(a'_1, r'_1), (a'_2, r'_2), \cdots, (a'_t, r'_t)\} \in (\mathcal{A} \times \mathbb{R})^t}
@p{a feasible t-history if it satisifies @inmath{Pr[H_t = H] > 0}, meaning that the probability of @inmath{H} to occur is not @inmath{0}. Also, if certain algorithm does generate a sequence @inmath{H} with non-zero probability, the algorithm is called H-consistent.}

@p{Thompson sampling is an algorithm selecting each arm with probability that the arm is the best one considering the history of all interactions. In each round, the algorithm calculates posterior distribution @inmath{Pr[a^* = a | H_{t-1} = H]} given the prior distribution @inmath{\mathbb{P}} and history up to the previous round @inmath{t-1}. Then, it chooses the arm by matching the probability, meaning the probability of choosing the arm @inmath{a} at round @inmath{t} is}
@blmath{ p_t(a | H) = Pr[a^* = a | H_{t-1} = H]. }
@p{Thompson sampling is known to achive the regret bound of}
@blmath{ BR(T) \le O(\sqrt{KT\ln(T)}). }
@p{The rest of this post will be devoted to the proof of this regret bound.}


@section{Presentation of the key lemma}

@p{The regret bounds of bandit algorithms are usually proved by using the Hoeffding inequality. Because the Thompson sampling is no exception, I will refresh the statement of the inequality briefly. Let @inmath{\{X_i\}_{i=1}^n} be a set of random variables such that @inmath{0 \le X_i \le 1} almost surely. The probability distribution of the sum of these variables @inmath{S_n}}
@blmath{S_n = \sum_{i=1}^n X_i}
@p{satisfies the following statement for all @inmath{t \ge 0}}
@blmath{ P( | S_n - \mathbb{E}[S_n] |  \ge t) \le 2 \exp(- \frac{2 t^2}{n}) .}
@p{In appendix A of A. Slivkins (2019), the author transforms the inequality for the ease of use in later discussions, by changing the focus of the inequality from the sum @inmath{S_n} to mean @inmath{\bar{X}_n}, and reparametrize the free parameter @inmath{t} with @inmath{T} satisfying }
@blmath{ \exp(- \frac{2 t^2}{n}) = \frac{1}{T^4}. }
@p{The result of these modifications is the equivalent, but different-looking inequality}
@blmath{ P( |\bar{X}_n - \mathbb{E}[X_n]| \ge \sqrt{\frac{2 \ln(T)}{n}} ) \le \frac{2}{T^4} }

@section{Proof of the regret bound}
@p{TBW...}
@blcode{
# plan...
- Introduction
  - It appears to be useful to understand the proofs for using the bandits for my work.
  - Here I will, describe, and prove.
- Description of the problem
  - General description of stochastic bandit problems
  - Description of bayesian bandit problems
  - The objective function of bayesian bandit problems
  - Simplifications of the prblem for ease of analysis
- Description of the algorithm
  - Some notations
  - Description of the thompson sampling algorithm
- Presentation of the key lemma
  - Hoeffding inequality
  - Proof of (3.14), (3.15) from hoeffding
- Proof of the regret bound
  - Lemma 3.10
  - the proof
}
