        <!doctype html>
            <html lang="ja">
            <head><meta charset="UTF-8"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-164492761-1"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag("js", new Date());gtag("config", "UA-164492761-1");</script><title>Preloading</title><link rel="stylesheet" href="../css/reset.css"><link rel="stylesheet" href="../css/style.css"><link rel="stylesheet" href="../css/vs.css"><script src="../js/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script><script src="https://unpkg.com/mermaid@8.5.2/dist/mermaid.min.js"></script><script>mermaid.initialize({startOnLoad:true});</script></head>
            <body>
                <div class="container">
                    <header class="header">
    <h1 class="logo">
        <a href="/">Preloading</a>
    </h1>
    <p class="desc">Ktakuyaのブログ</a>
</header>

                    <hr class="border">
                    <main class="main">
                        <div class="article-wrapper">
                            <div class="header">
                                <h2 class="title">backpropgationのまとめ</h2>
                                <p class="date">2018-11-29</h2>
                            </div>
                            <div class="content">
                                <p class="content-paragraph">色々なネット上の記事が neural network におけるbackpropagationをまとめているが、どの記法の自分が十分に気にいるものではなかったので、自分の気に入る記法でまとめてみる。導出の対象は通常の neural network のみが対象で、convolutionなどをするものは対象外としておく。</p>

<p class="content-paragraph">まずはnetworkを記述するところから始める。考察の対象とする neural network は第$0$層を入力層とする$N+1$層を持つものとし、各層$i$には$M_i$個のneuronが存在するとする。入力層を除くそれぞれの層$i \in \{1, 2, \cdots, N \}$の$k$番目のneuronの活性度$a^i_k$は一つ下の層のneuronの活性度$a^{i-1}_k$から次のように定まる。</p>

$$\begin{aligned}
z^i_k &=\sum_{j=1}^{M_{i-1}} w^i_{kj} a^{i-1}_j \\
a^i_k &= \phi_i(z^i_k)
\end{aligned}$$

<p class="content-paragraph">ただしここで$w^i_{kj}$は$i$層の$k$番目のneuronと$i-1$層の$j$番目のneuronをつなぐ枝の重みであり、$\phi_i$は$i$層目の活性化関数である。ここで活性化関数が層$i$に依存しているのは、出力層などを統一的に表すためである。最終的な出力$a^N$によってコスト関数$E(a^N)$が定まり、これを最小化することが最急降下法の目的となる。</p>

<p class="content-paragraph">では早速コスト関数$E$をパラメータで微分してみる。層$i$の枝$w^i_{nm}$に対する勾配は</p>

$$\begin{aligned}
\frac{\partial E}{\partial w^i_{nm}} &= \frac{\partial E}{\partial a^i_n} \frac{\partial a^i_n}{\partial z^i_n} \frac{\partial z^i_n}{\partial w^i_{nm}} \\
&= \frac{\partial E}{\partial a^i_n} \phi'_i(z^i_n) a^{i-1}_m
\end{aligned}$$

<p class="content-paragraph">であるが、ここで先と同様に</p>

$$\xi^i_n = \frac{\partial E}{\partial a^i_n} \phi'_i(z^i_n)$$

<p class="content-paragraph">とすれば、勾配は</p>

$$\frac{\partial E}{\partial w^i_{nm}} = \xi^i_n a^{i-1}_m$$

<p class="content-paragraph">と表せる。</p>


<p class="content-paragraph">最終層$i=N$については</p>

$$\begin{aligned}
\frac{\partial E}{\partial w^N_{nm}} &= \xi^N_n a^{N-1}_m \\
\xi^N_n &= \frac{\partial E}{\partial a^N_n} \phi'_i(z^N_n)
\end{aligned}$$

<p class="content-paragraph">となるが、$\partial E / \partial a^N_n$が簡単に計算できるので勾配計算は簡単に可能。</p>


<p class="content-paragraph">最終層以外については</p>

$$\begin{aligned}
\xi^i_n &= \phi'_i(z^i_n) \frac{\partial E}{\partial a^i_n} \\
&= \phi'_i(z^i_n) \sum_{k=1}^{M_{i+1}} \frac{\partial E}{\partial a^{i+1}_k} \frac{\partial a^{i+1}_k}{\partial a^i_n} \\
&= \phi'_i(z^i_n) \sum_{k=1}^{M_{i+1}} \frac{\partial E}{\partial a^{i+1}_k} \frac{\partial a^{i+1}_k}{\partial z^{i+1}_n} \frac{\partial z^{i+1}_k}{\partial a^i_n} \\
&= \phi'_i(z^i_n) \sum_{k=1}^{M_{i+1}} \frac{\partial E}{\partial a^{i+1}_k} \phi'_i(z^{i+1}_n) w^{i+1}_{kn} \\
&= \phi'_i(z^i_n) \sum_{k=1}^{M_{i+1}} \xi^{i+1}_k w^{i+1}_{kn}
\end{aligned}$$

<p class="content-paragraph">として、$\xi^{i+1}_k$を最終層から逆に求めていけば、</p>

$$\frac{\partial E}{\partial w^i_{nm}} = \xi^i_n a^{i-1}_m$$

<p class="content-paragraph">と計算できる。これでbackpropagationの導出ができた。</p>


<p class="content-paragraph">まとめれば、学習はForwardパスでは入力$a^0$に対して$i=1,2,\cdots,N$層まで</p>

$$\begin{aligned}
z^i_k &=\sum_{j=1}^{M_{i-1}} w^i_{kj} a^{i-1}_j \\
a^i_k &= \phi_i(z^i_k)
\end{aligned}$$

<p class="content-paragraph">として計算をしていくことで出力を得られる。Backwardパスではまず最上層の$\xi^N$を計算し</p>

$$\xi^N_n = \frac{\partial E}{\partial a^N_n} \phi'_i(z^N_n)$$

<p class="content-paragraph">それを元に、$i=N-1,N-2,\cdots,1$層に対して$\xi^i$を計算していく。</p>

$$\xi^i_n = \phi'_i(z^i_n) \sum_{k=1}^{M_{i+1}} \xi^{i+1}_k w^{i+1}_{kn}$$

<p class="content-paragraph">最後に、全ての層$i=1,2,\cdots,N$に対して勾配</p>

$$\frac{\partial E}{\partial w^i_{nm}} = \xi^i_n a^{i-1}_m$$

<p class="content-paragraph">を実際に計算し適用する。以上でコスト$E$を最小化するような学習を行うことができる。</p>
                            </div>
                        </div>
                    </main>
                    <hr class="border">
                    <footer class="footer">
    <p class="copyright">Copyright@2020 Ktakuya. All rights reserved.</p>
</footer>

                </div>
            </body>
        </html>
