# Rust(1987)におけるMDPの推定方法

最近この論文

- M.Igami (2015), [Estimating the Innovotr's Dilemma: Structual Analysis of Creative Destruction in the Hard Disk Drive Industry](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1733174)


を読むために、その解析の核心の部分を作り上げた論文

- J.Rust (1987), [Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zucher](https://www.jstor.org/stable/1911259?seq=1#page_scan_tab_contents)


を読んでいる。この論文の題名で調べるとその論文を読んでみましたとか授業で解説しましたなどの記事を多く見かけるので、多分有名な論文なのではないかと思う。

大まかな内容としては、あるバス会社におけるバスのメンテナンスを例にとり、動学的な離散選択モデルに対する推定問題のある一つの解き方を提案している。パスのメンテナンス、特にバスのエンジン交換は一度交換しようと思うととてもコストが高くなるが、コストが高いからといってエンジン交換を先延ばしにしていると日々のメンテナンスコストが上昇する上にいきなりエンジンが壊れてお客さんに迷惑をかけてしまう可能性もある。ただしあまり頻繁にエンジンを変えすぎてもそのコストが高くつくことになり、結果的に利益は出なくなってしまう。メンテナンスをする人はそのトレードオフをうまく考えて行っているはずであるが、そのような最適化を実際に行えているのだろうかという問いを立て、それに対してメンテナンスをする人の意思決定モデルを立ててそのパラメータを推定し妥当性を考察しているのがこの論文である。

今回はこの論文を読んで基本的な理論をまとめたようと思う。この論文で使用されているモデル、動学的離散選択モデルと今では言われているらしいが、とても強化学習と似通っている。そこで一度一般的な強化学習の推定問題だと思って以下に一般論をまとめてみた。

## 強化学習としてみた場合のRust(1987)

ここではあるマルコフ決定過程(MDP)上で常に最適な行動をとる行動主体の行動を観察することでMDP自体のパラメータを推定することを考える。ここで対象とする行動主体は必ずそのMDPに対して最適な行動をとっており、その学習はすでに終了していると考える。学習中の行動を観察することでMDPのパラメータを推定することが可能な方法もあるかもしれないが、少なくとも以下では扱わない。

このような問題設定は基本的には逆強化学習の問題設定と同一であるが、多くの逆強化学習の手法と異なる点は観察者が知らない情報があるという点を明示的にモデルに入れ込んでいる点である。今のところ自分は逆強化学習をあまりよく知らないので正確なことは言えないが、自分が知っている限りでは逆強化学習の手法はMDPの取りうる状態集合などを明確に知っていないといけないはずだ。このモデルは状態集合や報酬を明確に全て知っている必要がなく、観察者が知らない情報が比較的多く存在しても推定が可能である。

以下の議論では行動主体という言葉と観察者という言葉を明確に区別しておく必要がある。行動主体は実際にMDPと相互作用を行い状態を観察し行動を起こす主体である。そして観察者はその行動主体の行動を観察しその行動を分析する人のことを指す。観察者は行動主体が考えていることや行動主体が取得した情報全てを観察することができるとは限らず、その点で大まかに言えば観察者は行動主体よりも情報が制限されていると言える。

## 行動主体から見たMDPとその最適行動

強化学習に関しては一通り以前の記事で触れたので、最低限必要な点しか解説しない。まずはMDPの定義から行う。

> *マルコフ決定過程(MDP)*  
> マルコフ決定過程とは次の組$(S,A,P,R)$を指す。

- 状態集合と呼ばれる、MDPの取りうる状態の集合$S$
- 行動集合と呼ばれる、MDP上で行動主体が取りうる行動の有限集合$A$
- 遷移確率と呼ばれる、MDPがある状態$s \in S$の時に行動主体が行動$a \in A$をとった時にMDPの状態が$s'$に遷移する確率$P(s' | s, a)$
- MDPが状態$s \in S$である時に行動主体が行動$a \in A$をとった時に行動主体が得られる報酬$R(s, a)$


ついで、MDPがどの状態を最初に取り得るかどうかを表す確率分布$q(s)$を仮定することが多い。このようなMDPと相互作用する行動主体は次のように定義される。

> *行動主体*  
> 行動主体とは次の組$(\pi, \gamma)$を指す。

- 戦略と呼ばれる、MDPの各状態$s \in S$でどの行動$a \in A$をとるかを表す関数$\pi: S \rightarrow A$
- 割引率$\gamma \in [0, 1]$


行動主体の目的は次の期待報酬$J_\pi$を最大化するような戦略$\pi$を見つけることである。

$$
\begin{aligned}
J_\pi &= \sum_{s \in S} q(s) V_\pi (s) \\
V_\pi (s) &= \mathbb{E}_\pi [ \sum_{t=0}^{\infty} \beta^t R(s_t, a_t) | s_0 = s] \end{aligned}
$$


ただしここでの期待値$\mathbb{E}$は戦略$\pi$をとった際に起こりうる全てのMDPと行動主体の時間発展に対してとっている。このような期待報酬を最大化させる戦略$\pi^*$は最適価値関数

$$
V(s) = \max_\pi V_\pi (s)
$$


を使用して

$$
\begin{aligned}
\pi(s) &= {\rm argmax}_{a \in A} (R(s, a) + \gamma EV(s, a)) \\
EV(s, a) &= \sum_{s' \in S} P(s' | s, a) V(s')
\end{aligned}
$$


と表現できることが知られている。以降の議論では行動主体はこのような期待報酬を最大化するような戦略(最適戦略)を常にとっていると仮定して議論を進めていく。

## 観察者から見たMDPと推定のための仮定

観察者はMDPの状態の一部と行動主体がとった行動からMDPの構造を推定することが目標である。残念ながら観察者はこのような制限の中では、全く何の仮定もおかずにMDPを推測することはできず、少しだけ観察対象のMDPに対して仮定を置いて推定することになる。今回紹介する方法で必要になるMDPに対する仮定は以下のようになる。

> *MDPを推測するために観測者が必要な仮定*  
> 今回紹介する方法で観察者がMDPを推測するにあたって必要とするMDPに対する仮定は、上記のMDPの定義に加えて次の5つが必要となる。

- 状態集合$S$は観察者が観察可能な状態の集合$S_o$と観察者が観察不可能な状態の集合$S_u$の直積$S_o \times S_u$で表される。
- 行動主体の行動を観察者は全て観測できる。
- 遷移確率$P(s'|s,a)$は次のように分解でき、$P(s'_o | s_o, a)$は何らかのパラメトリックな分布$P(s'_o | s_o, a ; \theta_p)$で表現される。

  $$
\begin{aligned}
  P(s' | s, a) &= P(s'_o, s'_u | s_o, s_u, a) \\
  &= P(s'_u ) P(s'_o | s_o, a)
  \end{aligned}
$$
- 報酬は次のように観察者が観察可能な報酬$R_o$と観察者が観測不可能な報酬$R_u$に分解され、それぞれ観察可能な状態$s_o \in S_o$と観察不可能な状態$s_u \in S_u$にしか依存しない。そして、$R_o$は何かしらのパラメトリックな関数$R_o(s_o, a; \theta_r)$で表現され、$R_u$は上記で定められた$P(s'_u)$から導かれる分布が標準ガンベル分布${\rm Gumbel}(0, 1)$となると仮定する。

  $$
R(s, a) = R_o(s_o, a) + R_u(s_u, a)
$$
- 行動主体は常に最適戦略をとっている。


ここでいくつかこれらの仮定に対して注意しておくことをあげておく。まず遷移確率が分解できることについてだが、$P(s'_u)$の部分は依存性を少し残して$P(s'_u | s'_o)$としても以下の多くの議論は成り立つ。しかし今回は議論を簡潔にするために依存性を外してある。また、観測できない報酬$R_u$がガンベル分布をとるという仮定は単純に議論を簡潔にするためのものなので、他の分布になると仮定しても問題はない。ただし、この分布を使用すると議論が非常に簡潔になるため多くの文献で使用されている。

以上の仮定を置いた上で、観察者はMDPと行動主体を観察して得られたデータからMDPの情報を推定する。整理すれば以下のようになる。

> *観察者によるMDPの推定問題*  
> 観察者はMDPと行動主体について観察可能なものすべての有限時間$t = 0,1, \cdots, T$における歴史

$$
s_{o0}, a_0, s_{o1}, a_1, s_{o2}, a_2, \cdots, s_{oT}, a_T
$$


から遷移確率のパラメータ$\theta_p$と観察できる報酬のパラメータ$\theta_r$を推定する。ただしここで$s_{ot}$は時刻$t$での観察可能なMDPの状態を表し、$a_t$は同じく時刻$t$で行動主体がとった行動を表す。


## 観察者によるMDPの推定方法

MDPの推定は最尤推定を用いて行う。最尤推定であるから、MDPのパラメータ$\theta = (\theta_p, \theta_r)$がある適当な値を取った際の尤度が比較的明確な形でかけなければならない。その点において先の観察者によるMDPに対する過程の遷移確率に関する部分が効いてくる。データ$\{ s_{ot}, a_t\}_{t=0}^T$に対する尤度は

$$
\begin{aligned}
L(\{ s_{ot}, a_t\}_{t=0}^T) &= \prod_{t=0}^T P(s_{ot}, a_t | \{ s_{ot}, a_t\}_{t=0}^{t-1}; \theta) \\
&= \prod_{t=0}^T P(s_{ot}, a_t | s_{o(t-1)}, a_{t-1}; \theta)\\
&= \prod_{t=0}^T P(a_t | s_{ot}; \theta) P(s_{ot} | s_{o(t-1)}, a_{t-1}; \theta_p) \end{aligned}
$$


と分解できる。ここで一行目から二行目の式変形には遷移確率に対する仮定を使っている。よってlogにすれば

$$
\ln L(\{ s_{ot}, a_t\}_{t=0}^T) = \sum_{t=0}^T \ln P(a_t | s_{ot}; \theta) + \sum_{t=0}^T \ln P(s_{ot} | s_{o(t-1)}, a_{t-1}; \theta_p)
$$


となり、まず$\theta_p$を2項目を最適化することで推定し、その次にその結果を使って1項目を推定することでどちらのパラメータも推測して行くことができそうである。

よってまずは、2項目を使った$\theta_p$の推定を行うべきであるが、その推定は$P(s'_o | s_o, a; \theta_p)$として定めた関数の形によって大きく異なるのでここでは述べない。基本的にこの関数形があまりに複雑になっているとそもそもモデルとしての信頼性が損なわれる場合が多いはずなので簡単な関数形が選ばれることが多いはずであるから、何れにせよ推定は複雑ではない可能性が高い。

次に1項目を使用した$\theta_r$の推定であるが、これは少し複雑だ。そもそも$P(a_t | s_{ot}; \theta)$がどのように定まるものだったかというと、行動主体は最適戦略を常にとっているはずであるから必ず

$$
\begin{aligned} a_t &= {\rm argmax}_{a \in A} (R(s_t, a; \theta_r) + \gamma EV(s_t, a; \theta)) \\ &= {\rm argmax}_{a \in A} (R_o(s_{ot}, a; \theta_r) + R_u(s_{ut}, a) + \gamma EV(s_t,a; \theta)) \\ &= {\rm argmax}_{a \in A} (R_o(s_{ot}, a; \theta_r) + R_u(s_{ut}, a) + \gamma EV(s_{ot} ,a; \theta)) \end{aligned}
$$


となり決定論的に定まりそうだが、$R_u$が標準ガンベル分布にしたがう変数であるため決定論的には定まらなくなる。ただし2行目から3行目への式変形で

$$
\begin{aligned}
EV(s, a) &= \sum_{s' \in S} P(s' | s, a) V(s') \\ &= \sum_{s'_u \in S_u} P(s'_u) \sum_{s'_o \in S_o} P(s'_o | s_o, a) V(s'_o, s'_u) \\ &= \sum_{s'_u \in S_u} P(s'_u) \sum_{s'_o \in S_o} P(s'_o | s_o, a) V(s'_o) \\ &= \sum_{s'_o \in S_o} P(s'_o | s_o, a) V(s'_o) \\ &= EV(s_o, a)
\end{aligned}
$$


と書けることを使用している。導出が煩雑になりすぎるのでここでは最適価値関数から$s'_u$への依存性が消せることは示さずに使わせてもらった。具体的には$P(a_t | s_{ot}; \theta)$は[この記事](../build/basics-of-reinforcement-learning.html)で示したようにガンベル分布の性質から多項ロジットとなる

$$
P(a_t | s_{ot}; \theta) = \frac{\exp(R_o(s_{ot}, a_t; \theta_r) + \gamma EV(s_{ot}, a_t; \theta))}{\sum_{a \in A} \exp(R_o(s_{ot}, a_t; \theta_r) + \gamma EV(s_{ot}, a_t; \theta))}
$$


基本的にはこれの最尤推定を行えばいいのだが、ひとつわからないパラメータとして$EV(s_{ot}, a; \theta)$がある。この推定方法はいろいろあるだろうが、とりあえず次の性質を使用する方法がある。

> *期待最適価値関数についての性質*  
> 次の作用素$\mathcal{F}_\theta$を適当な実数関数$f: S_o \times A \rightarrow \mathbb{R}$に作用し続けると、$f$は期待最適価値関数$EV$に収束していく。

$$
\mathcal{F}_\theta f(s_o, a) = \sum_{s'_o \in S} P(s'_o | s_o, a; \theta_p) \ln [ \sum_{a' \in A} \exp(R_o(s'_o, a'; \theta_r) + \gamma f(s'_o, a'))]
$$


証明は長いので省略。いつか書くかも。とりあえず以前の記事の内容を使えば簡単なはず。

この性質を使えば、とりあえず数値的に$EV$を計算することはできる。あとは適当にパラメータ$\theta_r$をちらして尤度が高いものを取れば良い。

以上をまとめると、推定アルゴリズムは以下のようになる。

```
<katexがきちんと動いてないのはなんとかする>;
Given: $ \{ s_{ot}, a_t\}_{t=0}^T $
Output: $\theta_p, \theta_r$
Algorithm:
  尤度$ \sum_{t=0}^T \ln P(s_{ot} | s_{o(t-1)}, a_{t-1}; \theta_p) $が最大となる$\theta_p$を求める
  $\theta_r$の初期値を適当に定める
  while $\theta_r$が変化しなくなるまで do
    $\theta_r$の値を適当にばらつかせる。
    適当な関数$f: S_o \times A \rightarrow \mathbb{R}$を用意
    while $f$が変化しなくなるまで do
      $ f = \mathcal{F}_{(\theta_p, \theta_r)} f$
    end
    尤度$\sum_{t=0}^T \ln P(a_t | s_{ot}; \theta)$を$f$を$EV$の推定値として使って計算
    もし尤度が前回よりも高ければ$\theta_r$を更新
  end
  return $\theta_p, \theta_r$
```


このような$\theta_r$の推定方法を Nested Fixed Point Algrithm と呼ぶらしい。

## 参考文献

- Matthew Shum, [Dynamic Models: single agent problems](http://copeland.marginalq.com/tch_doc/dynamic_models.pdf), 2018年10月28日閲覧
