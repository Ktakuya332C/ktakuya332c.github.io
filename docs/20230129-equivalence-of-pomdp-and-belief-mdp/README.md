# POMDPとBeliefMDPの同値性

部分観測マルコフ決定過程(POMDP)を議論する際には、それと同値な信念空間におけるマルコフ決定過程(BeliefMDP)を構築することが多い。しかしその同値性の詳細な定義や証明などを記載している資料は少なく、代わりに次の論文が引用されいていることが多い。
- K.J Astrom, [Optimal control of Markov processes with incomplete state information](https://www.sciencedirect.com/science/article/pii/0022247X6590154X)

この論文は少し古いこともあり自分は読みづらい点もいくつかあったため、この記事では今後読み返す際にまた同じ疑問を抱かなくて済むよう、いくつかの点について注釈を残しておく。

## 問題の定義についての補足

2章「Statement of the problem」で定義されている各種確率変数の関係を整理すると次の図のようになる。

<img src="/20230129-equivalence-of-pomdp-and-belief-mdp/figure1.png">

ただしここで状態$\{x_t\}_{t=1}^{\infty}$の添字の始まりは、論文では0になっているがここでは1に変更している。論文中に書かれている通り添字を0始まりとして解釈すると、状態$x_0$から状態$x_1$に遷移する際に影響する制御変数$u(0)$が存在しないといけないが、制御変数$u(t)$は$t=1$以降に対してのみしか定義されておらず矛盾するからである。この論文中全ての箇所について状態の添字を0始まりから1始まりに変更しても矛盾する箇所はなかった。

## 同値性の定義とその証明の大まかな流れ

POMDPとBeliefMDPの同値性の定義と証明に焦点を当てると、この論文は次のような流れでそれらを説明している。
1. POMDPの定義を行う(2章)
1. 動的計画法を使って目的関数を最終時刻から1ステップごとに最適化できることを示し、それぞれの時刻で最適化される関数がその時刻における信念状態$w(t)$にのみ依存することを示す。これにより最適制御もどうように信念状態にのみ依存することを示す(3章)
1. POMDPに対応するBeliefMDPの定義を行い、この最適制御がPOMDPにおける最適制御と一致することを示す(4章)


この中で最も重要なステップは、3章で証明されている、それぞれの時刻で最適化される関数がその時刻における信念状態にのみ依存することを示すステップであり、最適制御を実現するためにはそれぞれの時刻で信念状態以外に過去のどんな情報も必要ないことが示せることで、等価なBeliefMDPというものを作成できるようになっている。
